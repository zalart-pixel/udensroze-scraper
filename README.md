# UDENSROZE Real Estate Scraper - Production System

Complete production-grade real estate scraping system for Puglia, Italy properties.

## üéØ System Overview

**What it does:**
- Scrapes properties from Italian real estate websites (immobiliare.it, idealista.it, gate-away.com)
- Evaluates each property against 100+ criteria (match % 0-100)
- Stores in Firestore for real-time querying
- Displays in interactive dashboard with live filtering
- Sends email alerts for CRITICAL matches (85%+)
- Runs automatically daily at 11 PM EET

**Architecture:**
```
Cloud Scheduler (11 PM EET)
    ‚Üì
Cloud Run: Scraper (30-60 min execution)
    ‚Üì
Firestore (real-time database) + Cloud Storage (backup)
    ‚Üì
Dashboard (Cloud Storage + CDN)
    ‚Üì
User Browser (properties.ian.tech)
```

**Cost: ~‚Ç¨4/month**

---

## üìã Prerequisites

- Google Cloud Project: `udensroze-scraper` (already created)
- GitHub repository: `zalart-pixel/udensroze-scraper` (already created)
- Domain: `ian.tech` with DNS access
- Gmail account for alerts

---

## üöÄ DEPLOYMENT GUIDE

### PHASE 1: Enable Google Cloud APIs (5 minutes)

```bash
# Set your project
gcloud config set project udensroze-scraper

# Enable all required APIs
gcloud services enable \
  run.googleapis.com \
  firestore.googleapis.com \
  storage.googleapis.com \
  cloudscheduler.googleapis.com \
  cloudbuild.googleapis.com \
  secretmanager.googleapis.com \
  logging.googleapis.com \
  monitoring.googleapis.com
```

**‚úÖ Verify:** Run `gcloud services list --enabled` - should see 8 APIs

---

### PHASE 2: Create Infrastructure (10 minutes)

```bash
# Create Firestore database
gcloud firestore databases create \
  --location=europe-west1 \
  --type=firestore-native

# Create Cloud Storage buckets
gsutil mb -l europe-west1 gs://udensroze-data
gsutil mb -l europe-west1 gs://udensroze-dashboard

# Make dashboard bucket public
gsutil iam ch allUsers:objectViewer gs://udensroze-dashboard
gsutil web set -m index.html gs://udensroze-dashboard

# Create service accounts
gcloud iam service-accounts create udensroze-scraper \
  --display-name="UDENSROZE Scraper"

# Grant Firestore + Storage permissions
gcloud projects add-iam-policy-binding udensroze-scraper \
  --member="serviceAccount:udensroze-scraper@udensroze-scraper.iam.gserviceaccount.com" \
  --role="roles/datastore.user"

gcloud projects add-iam-policy-binding udensroze-scraper \
  --member="serviceAccount:udensroze-scraper@udensroze-scraper.iam.gserviceaccount.com" \
  --role="roles/storage.objectAdmin"
```

**‚úÖ Verify:** 
- Check Firestore: `gcloud firestore databases list`
- Check buckets: `gsutil ls`

---

### PHASE 3: Setup Gmail Alerts (5 minutes)

**Step 1: Create App Password**
1. Go to: https://myaccount.google.com/apppasswords
2. Select app: "Mail"
3. Select device: "Other (Custom name)" ‚Üí "UDENSROZE Scraper"
4. Click "Generate"
5. Copy the 16-character password (e.g., `abcd efgh ijkl mnop`)

**Step 2: Store in Secret Manager**
```bash
# Store SMTP password (replace with your 16-char password)
echo -n "abcd efgh ijkl mnop" | \
  gcloud secrets create gmail-smtp-password \
  --data-file=- \
  --replication-policy="automatic"

# Grant access to service account
gcloud secrets add-iam-policy-binding gmail-smtp-password \
  --member="serviceAccount:udensroze-scraper@udensroze-scraper.iam.gserviceaccount.com" \
  --role="roles/secretmanager.secretAccessor"
```

**‚úÖ Verify:** `gcloud secrets list` - should see `gmail-smtp-password`

---

### PHASE 4: Upload Code to GitHub (5 minutes)

```bash
# Clone your repository
git clone https://github.com/zalart-pixel/udensroze-scraper.git
cd udensroze-scraper

# Copy all 7 production files to repo
# (Files are in /mnt/user-data/outputs/)

# Directory structure should be:
# udensroze-scraper/
# ‚îú‚îÄ‚îÄ scraper/
# ‚îÇ   ‚îú‚îÄ‚îÄ scraper.py
# ‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt
# ‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
# ‚îÇ   ‚îî‚îÄ‚îÄ .dockerignore
# ‚îú‚îÄ‚îÄ dashboard/
# ‚îÇ   ‚îú‚îÄ‚îÄ index.html
# ‚îÇ   ‚îî‚îÄ‚îÄ config.js
# ‚îî‚îÄ‚îÄ README.md

# Commit and push
git add .
git commit -m "Add production scraper and dashboard"
git push origin main
```

**‚úÖ Verify:** Visit https://github.com/zalart-pixel/udensroze-scraper

---

### PHASE 5: Deploy Scraper to Cloud Run (10 minutes)

```bash
cd scraper

# Build container image
gcloud builds submit --tag gcr.io/udensroze-scraper/scraper

# Deploy to Cloud Run (TEST MODE first)
gcloud run deploy udensroze-scraper \
  --image gcr.io/udensroze-scraper/scraper \
  --platform managed \
  --region europe-west1 \
  --memory 2Gi \
  --cpu 2 \
  --timeout 3600 \
  --max-instances 1 \
  --min-instances 0 \
  --service-account udensroze-scraper@udensroze-scraper.iam.gserviceaccount.com \
  --set-env-vars "TEST_MODE=true,GCP_PROJECT=udensroze-scraper,STORAGE_BUCKET=udensroze-data,SMTP_USER=your-email@gmail.com,ALERT_RECIPIENT=your-email@gmail.com" \
  --no-allow-unauthenticated

# Get service URL
gcloud run services describe udensroze-scraper --region europe-west1 --format 'value(status.url)'
```

**Note:** Replace `your-email@gmail.com` with your actual Gmail address

**‚úÖ Verify:** Check Cloud Run console - service should be deployed

---

### PHASE 6: Test Manual Scrape (5 minutes)

```bash
# Trigger manual test run (Monopoli only, 5 properties)
gcloud run jobs execute udensroze-scraper \
  --region europe-west1 \
  --wait

# View logs
gcloud logging read "resource.type=cloud_run_revision AND resource.labels.service_name=udensroze-scraper" \
  --limit 100 \
  --format "table(timestamp,textPayload)"
```

**Expected logs:**
```
UDENSROZE PROPERTY SCRAPER - STARTING
üß™ TEST MODE - Limited scraping
Target: 1 locations
üîç Scraping immobiliare.it for Monopoli
   ‚úÖ Found 5 properties in Monopoli
‚úÖ SCRAPING COMPLETE
Total properties: 5
CRITICAL: 1
üíæ Saved 5 properties to Firestore
‚òÅÔ∏è Saved to Cloud Storage
üìß Alert sent: ‚úÖ UDENSROZE Scrape Complete
```

**‚úÖ Verify:**
1. Check Firestore: https://console.firebase.google.com/project/udensroze-scraper/firestore
   - Should see `properties` collection with 5 documents
2. Check email: Should receive completion alert
3. Check Cloud Storage: `gsutil ls gs://udensroze-data/latest/`
   - Should see `properties.json`

---

### PHASE 7: Setup Daily Automation (5 minutes)

```bash
# Turn off test mode (enable full scraping)
gcloud run services update udensroze-scraper \
  --region europe-west1 \
  --set-env-vars "TEST_MODE=false,GCP_PROJECT=udensroze-scraper,STORAGE_BUCKET=udensroze-data,SMTP_USER=your-email@gmail.com,ALERT_RECIPIENT=your-email@gmail.com"

# Create Cloud Scheduler job (11 PM EET = 9 PM UTC)
gcloud scheduler jobs create http udensroze-daily-scrape \
  --location europe-west1 \
  --schedule "0 21 * * *" \
  --uri "$(gcloud run services describe udensroze-scraper --region europe-west1 --format 'value(status.url)')" \
  --http-method POST \
  --oidc-service-account-email udensroze-scraper@udensroze-scraper.iam.gserviceaccount.com \
  --time-zone "Europe/Athens" \
  --attempt-deadline 3600s
```

**‚úÖ Verify:** 
```bash
# List scheduler jobs
gcloud scheduler jobs list --location europe-west1

# Test run manually (optional)
gcloud scheduler jobs run udensroze-daily-scrape --location europe-west1
```

---

### PHASE 8: Deploy Dashboard (10 minutes)

**Step 1: Configure Firebase**

1. Go to Firebase Console: https://console.firebase.google.com/project/udensroze-scraper
2. Click "Add app" ‚Üí Web (</> icon)
3. App nickname: "UDENSROZE Dashboard"
4. Click "Register app"
5. Copy the `firebaseConfig` object
6. Edit `dashboard/config.js` - paste your config

**Step 2: Update Firestore Security Rules**

1. Go to: https://console.firebase.google.com/project/udensroze-scraper/firestore/rules
2. Replace with:

```javascript
rules_version = '2';
service cloud.firestore {
  match /databases/{database}/documents {
    match /properties/{property} {
      allow read: if true;   // Public read
      allow write: if false; // Only scraper can write
    }
    match /scrape_runs/{run} {
      allow read: if true;
      allow write: if false;
    }
  }
}
```

3. Click "Publish"

**Step 3: Deploy to Cloud Storage**

```bash
cd dashboard

# Upload dashboard files
gsutil -m cp index.html config.js gs://udensroze-dashboard/

# Test dashboard URL
echo "Dashboard: https://storage.googleapis.com/udensroze-dashboard/index.html"
```

**Step 4: Setup Custom Domain**

1. Open your DNS provider (e.g., Cloudflare, GoDaddy)
2. Add CNAME record:
   ```
   Name: properties
   Value: c.storage.googleapis.com
   TTL: Auto
   ```

3. Verify DNS propagation (may take 5-60 minutes):
   ```bash
   nslookup properties.ian.tech
   ```

4. Configure Cloud Load Balancer (optional, for HTTPS):
   ```bash
   # Create backend bucket
   gcloud compute backend-buckets create udensroze-dashboard-backend \
     --gcs-bucket-name=udensroze-dashboard \
     --enable-cdn
   
   # Create URL map
   gcloud compute url-maps create udensroze-dashboard-map \
     --default-backend-bucket=udensroze-dashboard-backend
   
   # Create managed SSL certificate
   gcloud compute ssl-certificates create udensroze-dashboard-cert \
     --domains=properties.ian.tech \
     --global
   
   # Create HTTPS proxy
   gcloud compute target-https-proxies create udensroze-dashboard-proxy \
     --url-map=udensroze-dashboard-map \
     --ssl-certificates=udensroze-dashboard-cert
   
   # Create forwarding rule
   gcloud compute forwarding-rules create udensroze-dashboard-https \
     --global \
     --target-https-proxy=udensroze-dashboard-proxy \
     --ports=443
   
   # Get IP address
   gcloud compute forwarding-rules describe udensroze-dashboard-https --global --format 'value(IPAddress)'
   ```

5. Update DNS A record:
   ```
   Name: properties
   Type: A
   Value: <IP from above>
   ```

**‚úÖ Verify:** Visit https://properties.ian.tech - should see dashboard

---

### PHASE 9: Setup Monitoring (10 minutes)

**Step 1: Create Uptime Check**

```bash
# Create uptime check for dashboard
gcloud monitoring uptime-checks create \
  --display-name="UDENSROZE Dashboard" \
  --resource-type=uptime-url \
  --host=properties.ian.tech \
  --path=/ \
  --port=443 \
  --protocol=https \
  --timeout=10s \
  --check-interval=300s
```

**Step 2: Create Notification Channel**

```bash
# Create email notification channel
gcloud alpha monitoring channels create \
  --display-name="UDENSROZE Alerts" \
  --type=email \
  --channel-labels=email_address=your-email@gmail.com
```

**Step 3: Create Alert Policies**

1. Go to: https://console.cloud.google.com/monitoring/alerting/policies/create?project=udensroze-scraper

2. Create alert: "Scraping Failed"
   - Condition: Cloud Run revision > Error rate > 50%
   - Duration: 5 minutes
   - Notification: Email

3. Create alert: "Dashboard Down"
   - Condition: Uptime check fails
   - Duration: 10 minutes
   - Notification: Email

**‚úÖ Verify:** Check Monitoring dashboard for uptime check

---

## üìä System Verification Checklist

After deployment, verify everything works:

- [ ] Firestore has `properties` collection
- [ ] Cloud Storage has `latest/properties.json`
- [ ] Dashboard loads at https://properties.ian.tech
- [ ] Dashboard displays properties from Firestore
- [ ] Filters work in real-time
- [ ] Email alerts received after scrape
- [ ] Cloud Scheduler job configured (11 PM EET)
- [ ] Monitoring uptime checks active
- [ ] Logs visible in Cloud Logging

---

## üîß Configuration

### Environment Variables

Set in Cloud Run service:

- `TEST_MODE`: `true` or `false` (default: false)
- `GCP_PROJECT`: `udensroze-scraper`
- `STORAGE_BUCKET`: `udensroze-data`
- `SMTP_USER`: Your Gmail address
- `ALERT_RECIPIENT`: Email for alerts

### Scraper Settings

Edit `scraper/scraper.py` - CONFIG section:

```python
CONFIG = {
    'locations': [...],  # 12 Puglia locations
    'max_properties_per_location': 50,
    'delay_between_requests': 3,  # seconds
    'delay_between_sites': 5,
}
```

### Evaluation Criteria

Edit `scraper/scraper.py` - PropertyEvaluator.CRITERIA:

```python
CRITERIA = {
    'geographic': {'weight': 0.30},  # 30%
    'land_space': {'weight': 0.25},  # 25%
    'architectural': {'weight': 0.15}, # 15%
    'infrastructure': {'weight': 0.15},
    'regulatory': {'weight': 0.10},
    'financial': {'weight': 0.05}
}
```

---

## üêõ Troubleshooting

### Scraper fails to run

**Check logs:**
```bash
gcloud logging read "resource.type=cloud_run_revision" --limit 50
```

**Common issues:**
- Timeout: Increase `--timeout` in Cloud Run
- Memory: Increase `--memory` to 4Gi
- Rate limiting: Increase delays in CONFIG

### Dashboard shows no properties

**Check Firestore:**
1. Visit Firebase Console
2. Check `properties` collection exists
3. Check documents have data

**Check config.js:**
1. Verify Firebase config is correct
2. Check Firestore security rules allow read

### Email alerts not working

**Check Secret Manager:**
```bash
gcloud secrets versions access latest --secret="gmail-smtp-password"
```

**Verify Gmail App Password:**
- Must be 16 characters
- Must be generated from Google Account settings
- 2FA must be enabled

### Scheduler not running

**Check job:**
```bash
gcloud scheduler jobs list --location europe-west1
```

**Test manually:**
```bash
gcloud scheduler jobs run udensroze-daily-scrape --location europe-west1
```

---

## üìà Monitoring & Maintenance

### Daily Checks

- Check email for scrape completion alert
- Verify property count increasing
- Check for CRITICAL matches

### Weekly Checks

- Review Cloud Run logs for errors
- Check Firestore storage usage
- Verify dashboard loads quickly

### Monthly Checks

- Review and adjust evaluation criteria
- Update property search URLs if websites change
- Clean up old data in Cloud Storage

### Costs

Monitor spending:
```bash
gcloud billing accounts list
```

Expected monthly costs:
- Cloud Run: ‚Ç¨4
- Firestore: Free (within free tier)
- Cloud Storage: ‚Ç¨0.10
- Cloud Scheduler: Free
- **Total: ~‚Ç¨4/month**

---

## üöÄ Next Steps

### Add More Scrapers

Edit `scraper/scraper.py` - add new scraper classes:

```python
class IdealistaScraper:
    """Scraper for idealista.it"""
    # ... implementation

class GateAwayScraper:
    """Scraper for gate-away.com"""
    # ... implementation
```

Then add to PropertyScraper:
```python
self.scrapers = [
    ImmobiliareScraper(),
    IdealistaScraper(),  # NEW
    GateAwayScraper()    # NEW
]
```

### Add Price Change Tracking

Store historical prices in Firestore:

```python
# In PropertyScraper._save_to_firestore()
if previous_price and previous_price != prop['price']:
    db.collection('price_history').add({
        'property_id': prop['id'],
        'old_price': previous_price,
        'new_price': prop['price'],
        'change': prop['price'] - previous_price,
        'timestamp': firestore.SERVER_TIMESTAMP
    })
```

### Add Image Scraping

Download and store property images:

```python
# In scraper class
images = self._download_images(listing)
prop['images'] = self._upload_to_storage(images)
```

---

## üìö Resources

- **Firestore Console:** https://console.firebase.google.com/project/udensroze-scraper/firestore
- **Cloud Run Console:** https://console.cloud.google.com/run?project=udensroze-scraper
- **Cloud Storage:** https://console.cloud.google.com/storage/browser?project=udensroze-scraper
- **Cloud Logging:** https://console.cloud.google.com/logs?project=udensroze-scraper
- **Cloud Monitoring:** https://console.cloud.google.com/monitoring?project=udensroze-scraper
- **Cloud Scheduler:** https://console.cloud.google.com/cloudscheduler?project=udensroze-scraper

---

## üìù License

Private project - All rights reserved.

---

## ‚úÖ SUCCESS!

Your UDENSROZE real estate scraping system is now production-ready.

**What happens next:**
1. Tonight at 11 PM EET: Scraper runs automatically
2. 30-60 minutes later: 100-300 properties collected
3. Dashboard updates in real-time
4. Email alert sent with CRITICAL matches
5. Repeat daily forever

**Cost:** ~‚Ç¨4/month  
**Maintenance:** ~5 minutes/week  
**Value:** Priceless property insights üèõÔ∏èüåä

**Happy property hunting!** üöÄ
